# evaluation
do_eval: true
eval_strategy: "no"
eval_steps: 200
load_best_model_at_end: False

# saving
save_strategy: "steps"
save_steps: 200
save_total_limit: 5

# logging
logging_strategy: "steps"
logging_steps: 2

# training
num_train_epochs: 10000
weight_decay: 0.1
lr_scheduler_type: "cosine"
# lr_scheduler_kwargs: {
#   "patience": 1,
#   "factor": 0.5,
#   "mode": null,
# }
adam_beta1: 0.9
adam_beta2: 0.995
gradient_checkpointing: False
gradient_checkpointing_kwargs: { "use_reentrant": False }
optim: "adamw_torch_fused"
warmup_ratio: 0.2
ddp_timeout: 30000
bf16: True
fp16: False

# dataset
dataloader_num_workers: 4
remove_unused_columns: false

push_to_hub: false
report_to: [ "wandb" ]
overwrite_output_dir: True
seed: 42
data_seed: 42